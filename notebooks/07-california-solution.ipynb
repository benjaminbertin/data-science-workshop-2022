{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Etude du jeu de données California\n",
    "\n",
    "Ce dataset contient la valeur des logements en Californie par district.\n",
    "\n",
    "Nous cherchons à prédire la variable `median_house_value`, cette variable étant numérique et continue, nous avons à faire à un problème de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math, pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Analyse exploratoire des données\n",
    "\n",
    "### 7.2.1 Statistiques\n",
    "\n",
    "Dimensions de notre dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrait :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types des variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description des variables numériques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description des variables catégorielles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=[object]).columns:\n",
    "    counts = df[col].value_counts()\n",
    "    if len(counts) < 20:\n",
    "        print(\"\\nModalités de la variable \", col)\n",
    "        pprint.pprint(df[col].value_counts())\n",
    "    else:\n",
    "        print(\"\\nLa variable %s possède %i modalités\" % (col, len(counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des corrélations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(df).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boîtes à moustaches - version sns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_grid_sns(df, nb_grid_cols=3, figsize=(16,15)):\n",
    "    df_num = df.select_dtypes(include=[np.number])\n",
    "    nb_num_vars = len(df_num.columns)\n",
    "    fig, axes = plt.subplots(math.ceil(nb_num_vars/nb_grid_cols), nb_grid_cols, figsize=figsize)\n",
    "    for i, col in enumerate(df_num.columns):\n",
    "        sns.boxplot(data=df_num, x=col, ax=axes[int(i/nb_grid_cols),i%nb_grid_cols])\n",
    "    plt.show()\n",
    "boxplot_grid_sns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boîtes à moustaches - version plotly :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_grid_plotly(df, nb_grid_cols=3):\n",
    "    cols = df.select_dtypes(include=[np.number]).columns\n",
    "    fig = make_subplots(rows=math.ceil(len(cols)/nb_grid_cols), cols=nb_grid_cols)\n",
    "    for i, var in enumerate(cols):\n",
    "        fig.add_trace(\n",
    "            go.Box(y=df[var],\n",
    "            name=var),\n",
    "            row=int(i/nb_grid_cols)+1, col=i%nb_grid_cols+1\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=950,\n",
    "        height=800,\n",
    "        showlegend=False\n",
    "    )\n",
    "    fig.show()\n",
    "boxplot_grid_plotly(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogrammes avec estimation de densité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_plot_grid(df, nb_grid_cols=3, figsize=(16,15)):\n",
    "    df_num = df.select_dtypes(include=[np.number])\n",
    "    nb_num_vars = len(df_num.columns)\n",
    "    fig, axes = plt.subplots(math.ceil(nb_num_vars/nb_grid_cols), nb_grid_cols, figsize=figsize)\n",
    "    for i, col in enumerate(df_num.columns):\n",
    "        not_null_col = df_num[col][df_num[col].notnull()]\n",
    "        sns.histplot(not_null_col, ax=axes[int(i/nb_grid_cols),i%nb_grid_cols], kde=True)\n",
    "    plt.show()\n",
    "density_plot_grid(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il y a des valeurs manquantes pour la variable `total_bedrooms`. Nous traiterons les valeurs manquantes dans la chaîne de transformation pour le pré-traitement des données.\n",
    "- Les valeurs de la cible `median_house_value` supérieures à `500000` semblent avoir subi un effet de seuil et avoir été enregistrées avec un montant associé fixé à `500000`. Il semble préférable de ne pas conserver ces enregistrements. Par ailleurs, observons qu'ils se concentrent sur des zones bien spécifiques de la Californie.\n",
    "- Les variables `total_rooms`, `total_bedrooms`, `population` et `households` sont très fortement corrélées. Il semble pertinent de rapporter le nombre de pièces et la population au nombre de foyers. De même, le nombre de chambres peut être ramené au nombre de pièces. Nous proposons d'introduire les variables `rooms_per_household`, `population_per_household` et `bedrooms_per_room`.\n",
    "- Certaines boîtes à moustaches indiqueraient un grand nombre de valeurs aberrantes. Cependant, les histogrammes avec estimateur de densité montrent que les distributions correspondantes ne sont pas normales mais asymétriques avec plus d'observations du côté des valeurs élevées.\n",
    "- Une standardisation ou normalisation sont à tester, en particulier dans le cas de modèles linéaires, car les échelles des variables sont très différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.2 Étude des prix de maisons médians supérieurs à 500 000\n",
    "\n",
    "Commencons par afficher le nombre de lignes pour lesquels la `median_house_value` a été seuillée à 500k :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['median_house_value']>500000).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutons une colonne indiquant si les lignes ont un prix seuillé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['500k'] = df['median_house_value']>500000\n",
    "df['500k'] = df['500k'].astype(int);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis affichons les données sur une carte avec une couleur différente pour les districts qui ont vu leur `median_house_value` seuillée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.3,\n",
    "        s=df[\"population\"]/100, label=\"population\",\n",
    "        figsize=(10,8), c=\"500k\", cmap=plt.get_cmap(\"rainbow\"))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces districts semblent être localisés autour des villes de Los Angeles et San Fransisco, ces lignes seront potentiellement génantes pour apprendre un regresseur, aussi nous les supprimons (et nous supprimons la colonne `500k` qui ne sera plus utile par la suite) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['500k']);\n",
    "df = df.drop(df[df.median_house_value > 500000].index)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2.3 Étude des valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une stratégie possible pour la gestion des valeurs manquantes serait de supprimer les lignes correspondantes. Voir ci-dessous une manière de le faire. Cependant, nous préférons gérer les valeurs manquantes au sein de la chaîne de transformation qui opérera l'ensemble des pré-traitements. Ainsi, nous pouvons comparer par validation croisée différentes formes d'imputation pour la gestion des valeurs manquantes. Cette approche est également plus robuste si, pour la mise en production du modèle, d'autres variables que `total_bedrooms` ont des valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['total_bedrooms'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 Transformation des variables fortement corrélées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous calculons les nouvelles variables `rooms_per_household`, `population_per_household` et `bedrooms_per_room`. Nous retirons les variables `total_rooms`, `total_bedrooms` et `population`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rooms_per_household'] = df['total_rooms'] / df['households']\n",
    "df['bedrooms_per_room'] = df['total_bedrooms'] / df['total_rooms']\n",
    "df['population_per_household'] = df['population'] / df['households']\n",
    "df = df.drop(columns=['total_rooms', 'total_bedrooms', 'population'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que le revenu médian est corrélé positivement avec le nombre de pièces mais négativement avec la proportion de chambres à coucher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.5 Visualisations géographiques\n",
    "\n",
    "Nous pouvons afficher les individus de notre dataset sur un plan dont les axes sont la longitude et la latitude :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette représentation peut-être améliorée en changeant la couleur des points en fonction de la `median_house_value` et la taille en fonction de la `population_per_household` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.3,\n",
    "        s=df[\"population_per_household\"], label=\"population per household\",\n",
    "        figsize=(10,8), c=\"median_house_value\",\n",
    "        cmap=plt.get_cmap(\"rainbow\"), colorbar=True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique précédent nous permet de constater qu'il y a probablement une erreur sur certains districts car ils font apparaitre des `population_per_household` très grand. Regardons cela plus en détail :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.population_per_household > 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces lignes seront problématique pour notre regresseur, aussi nous choisissons de les supprimer en choisissant un seuil au dela duquel les valeurs nous semblent déraisonnables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df.population_per_household > 20].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Construction du jeu d'entraînement et du jeu de test\n",
    "\n",
    "Après cette étape d'analyse exploratoire, de transformation de variables et de nettoyage de nos données, nous pouvons maintenant séparer notre dataset en une partie d'entrainement et une partie de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=77)\n",
    "\n",
    "X_train = train.drop(\"median_house_value\", axis=1)\n",
    "y_train = train[\"median_house_value\"].copy()\n",
    "X_test = test.drop(\"median_house_value\", axis=1)\n",
    "y_test = test[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Transformation des variables\n",
    "\n",
    "### 7.4.1 Classement des variables en fonction de leurs types (numériques, catégorielles, ordinales...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names_num = ['longitude', 'latitude', 'housing_median_age', \n",
    "                 'households', 'median_income', \n",
    "                 'rooms_per_household', 'bedrooms_per_room', 'population_per_household' ]\n",
    "var_names_cat = ['ocean_proximity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 Création des chaînes de transformation pour la préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preparation_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, var_names_num),\n",
    "    (\"cat\", OneHotEncoder(), var_names_cat),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Exploration des modèles\n",
    "\n",
    "### 7.5.1 Modèle Ridge\n",
    "\n",
    "#### Choix des hyper-paramètres par validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_pipeline = Pipeline([\n",
    "    (\"preparation\", preparation_pipeline),\n",
    "    (\"model\", Ridge(tol=0.01, solver=\"saga\"))\n",
    "])\n",
    "\n",
    "ridge_param_grid = [\n",
    "    {\n",
    "        'model__alpha': np.logspace(-5, 5, 10),\n",
    "        'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent']\n",
    "    }\n",
    "]\n",
    "\n",
    "gridsearch_ridge = GridSearchCV(\n",
    "    ridge_pipeline, ridge_param_grid, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    ")\n",
    "gridsearch_ridge.fit(X_train, y_train)\n",
    "\n",
    "print('Meilleurs paramètres :')\n",
    "print(gridsearch_ridge.best_params_)\n",
    "\n",
    "rmse_best_ridge_model = np.sqrt(-gridsearch_ridge.best_score_)\n",
    "print('RMSE par validation croisée du meilleur modèle : ', rmse_best_ridge_model)\n",
    "\n",
    "best_ridge_model = gridsearch_ridge.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde du meilleur modèle\n",
    "\n",
    "La cellule suivante sauvegarde le modèle (i.e. : tout le pipeline) dans un fichier qui pourra être réutilisé ultiérieurement (voir à la fin du notebook pour un exemple d'utilisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "dump(best_ridge_model, 'california_ridge_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score par validation croisée sur le jeu d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv_scores = cross_val_score(\n",
    "    best_ridge_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1\n",
    ")\n",
    "ridge_cv_scores = np.sqrt(-ridge_cv_scores)\n",
    "print(\n",
    "    \"Moyenne des scores : \", ridge_cv_scores.mean(),\n",
    "    \"Ecart type des scores : \", ridge_cv_scores.std()\n",
    ")\n",
    "dump(ridge_cv_scores, 'california_ridge_score.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.2 Modèle de forêt aléatoire\n",
    "\n",
    "#### Choix des hyper-paramètres par validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preparation\", preparation_pipeline),\n",
    "    (\"model\", RandomForestRegressor(n_jobs=-1, n_estimators=50, random_state=77, criterion='mse'))\n",
    "])\n",
    "\n",
    "rf_param_grid = [\n",
    "    {\n",
    "        'preparation__num__imputer__strategy': ['mean', 'median', 'most_frequent'],\n",
    "        'model__min_samples_leaf': [1,3,5],\n",
    "        'model__max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "]\n",
    "\n",
    "gridsearch_rf = GridSearchCV(rf_pipeline, rf_param_grid, cv=3, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "gridsearch_rf.fit(X_train, y_train)\n",
    "\n",
    "print('Meilleurs paramètres :')\n",
    "print(gridsearch_rf.best_params_)\n",
    "\n",
    "rmse_best_rf_model = np.sqrt(-gridsearch_rf.best_score_)\n",
    "print('RMSE par validation croisée du meilleur modèle : ', rmse_best_rf_model)\n",
    "\n",
    "best_rf_model = gridsearch_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde du meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(best_rf_model, 'california_rf_model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de l'importance des variables\n",
    "\n",
    "Une forêt aléatoire nous permet de savoir quelles sont les variables qui ont le plus d'impact sur la prédiction. Les cellules suivantes permettent d'afficher un graphique de l'importance des variables dans la décision du prédicteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names_one_hot = list(best_rf_model.named_steps[\"preparation\"].named_transformers_['cat'].categories_[0])\n",
    "var_names_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = var_names_num + var_names_one_hot\n",
    "var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importances(rf_model, var_names, figsize=(12,12)):\n",
    "    feature_importances = rf_model.feature_importances_\n",
    "    std_feature_importances = np.std([tree.feature_importances_ for tree in rf_model.estimators_], axis=0)\n",
    "    print(sorted(zip(feature_importances, std_feature_importances, var_names), reverse=True))\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    sorted_names = [name for _ , name in sorted(zip(feature_importances, var_names), reverse=True)]\n",
    "    nb_features = len(feature_importances)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"Importance des variables\")\n",
    "    plt.bar(range(nb_features), feature_importances[sorted_indices],\n",
    "            color=\"b\", yerr=std_feature_importances[sorted_indices], \n",
    "            align=\"center\")\n",
    "    plt.xticks(range(nb_features), sorted_names, rotation=70)\n",
    "    plt.xlim([-1, nb_features])\n",
    "    plt.show()\n",
    "feature_importances(best_rf_model.named_steps['model'], var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score par validation croisée sur le jeu d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv_scores = cross_val_score(\n",
    "    best_rf_model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10, n_jobs=-1\n",
    ")\n",
    "rf_cv_scores = np.sqrt(-rf_cv_scores)\n",
    "print(\n",
    "    \"Moyenne des scores : \", rf_cv_scores.mean(),\n",
    "    \"Ecart type des scores : \", rf_cv_scores.std()\n",
    ")\n",
    "dump(rf_cv_scores, 'california_rf_score.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.3 Comparaison des modèles\n",
    "\n",
    "Nous pouvons afficher les scores obtenus par les différents modèles sur les validations croisées effectuées jusqu'ici. Pour choisir un modèle, nous regardons aussi bien les performances moyennes de chaque modèle que l'écart type de leurs performances. Si nous avons plusieurs modèles sensiblement identiques en terme de performance, nous privilegierons celui qui a l'écart type le plus faible pour minimiser le risque d'être confronté à une variance importante lors des tests sur le dataset de test (et lors de l'utilisation en production du modèle retenu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "axis = figure.add_subplot(111)\n",
    "plt.boxplot([ridge_cv_scores, rf_cv_scores])\n",
    "axis.set_xticklabels(['Ridge', 'Forêt aléatoire'], rotation = 45, ha=\"right\")\n",
    "axis.set_ylabel(\"Root Mean Squared Error (RMSE)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Evaluation du meilleur modèle sur le jeu de test\n",
    "\n",
    "Finalement nous testons ce modèle sur nos données de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_pred = best_rf_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
    "print(\"Test RMSE : \", test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de réutilisation d'un modèle enregistré (pourrait être dans un script Python séparé ou dans un autre notebook) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('california_rf_model.joblib')\n",
    "clf.predict(X_test.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
