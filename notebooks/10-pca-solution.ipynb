{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dfb7d2d",
   "metadata": {},
   "source": [
    "# 10. ACP\n",
    "\n",
    "Dans ce notebook, nous allons appliquer une ACP sur différents datasets en utilisant la librairie [scikit-learn](https://scikit-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9834397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale, normalize, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8939f30",
   "metadata": {},
   "source": [
    "## 10.1 ACP pour la compréhension et l'analyse d'un dataset\n",
    "\n",
    "Nous allons appliquer une ACP sur un jeu de données décrivant quelques statistiques sur des pays. L'objectif est d'identifier de trouver une catégorisation des pays en fonction de ces donnés.\n",
    "\n",
    "Charger le dataset contenu dans le fichier `data/country-data.csv` dans une dataframe pandas, visualisez un échantillon de ces données, afficher quelques statistiques (mean, std...) et donnez sa taille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef67d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/country-data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5fa8ff",
   "metadata": {},
   "source": [
    "Nous nous intéressons à présent aux corrélations linéaires entre les variables de ce dataset à l'aide de la méthode [`pandas.DataFrame.corr`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html).\n",
    "\n",
    "Afficher la matrice des corrélations avec la méthode [`sns.heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html). Vous pouvez utiliser le code `plt.figure(figsize=(10,10))` au préalable pour obtenir une figure plus lisible. Pour afficher cette matrice, les paramètres intéressants de `sns.heatmap` sont `square`, `annot` et `cmap` (utilisez l'espace de couleur `sns.diverging_palette(20, 220, n=200)` pour plus de lisibilité)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9950d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(correlation, vmax=1, square=True,annot=True, cmap=sns.diverging_palette(20, 220, n=200))\n",
    "\n",
    "plt.title('Correlation between different fearures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a521d",
   "metadata": {},
   "source": [
    "Afficher les individus du dataset pour quelques paires de variables fortement corrélées. Vous pouvez utiliser un [scatter plot plotly](https://plotly.com/python/line-and-scatter/). \n",
    "\n",
    "Quelles conclusions pouvezèvous tirer des variables et des individus ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc7958",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"child_mort\", y=\"life_expec\", text=\"country\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df92bbf",
   "metadata": {},
   "source": [
    "Créez une variable `X` contenant les données numériques des individus (i.e. : sans la colonne `country`). Puis appliquez la méthode `fit_transform` de la classe [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) de scikit-lean, stockez le résultat dans une nouvelle variable `X_std`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b278f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32afbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)\n",
    "# df_std = StandardScaler().fit_transform(df.drop(columns='country'))\n",
    "X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9658856",
   "metadata": {},
   "source": [
    "Afficher la distribution des variables de ce dataset avant et après la standardisation en utilisant [`pandas.DataFrame.plot.density`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.density.html) (les paramètres `sharex=True,figsize=(12,5)` rendront ce graphique plus lisible).\n",
    "\n",
    "Sur le graphique de la distribution des variables avant standardisation, certaines variables dont la plage des valeurs est trop différentes des autres rendrait ce graphique illisible. Pensez à afficher un graphique différent pour chaque groupe de variables ayant une plage de valeurs proche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['income', 'gdpp']).plot.density(sharex=True,figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198bc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['income', 'gdpp']].plot.density(sharex=True,figsize=(12,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns='country').columns.tolist()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff107b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(X_std, columns=features).plot.density(sharex=True,figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57c91ac",
   "metadata": {},
   "source": [
    "Appliquez la méthode `fit` de la classe [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) pour calculer les composantes principales. Puis afficher les valeurs des attributs `components_`, `explained_variance_` et `explained_variance_ratio_`. A quoi correspondent ces attributs ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcd748",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbf92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4316a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede74593",
   "metadata": {},
   "source": [
    "L'attribut `components_` contient les coefficients des combinaisons linéaires des différentes composantes principales, l'attribut `explained_variance_` fournit la variance expliquée sur le dataset pour chacune des composantes principales et l'attribut `explained_variance_ratio_` la proportion de variance expliquée cumulée par les composantes principales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bac2eb",
   "metadata": {},
   "source": [
    "La fonction suivante vous permet d'afficher un graphique représentant la variance expliquée par les différentes composantes principales et la variance cumulée. Utilisez cette fonction sur votre ACP. Combien faut-il conserver de cmposantes principales pour expliquer plus de 80% de la variabilité des données ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scree_plot(pca):\n",
    "    '''Display a scree plot for the pca'''\n",
    "\n",
    "    scree = pca.explained_variance_ratio_*100\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.bar(np.arange(len(scree))+1, scree)\n",
    "    plt.plot(np.arange(len(scree))+1, scree.cumsum(),c=\"red\",marker='o')\n",
    "    plt.xlabel(\"Number of principal components\")\n",
    "    plt.ylabel(\"Percentage explained variance\")\n",
    "    plt.title(\"Scree plot\")\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f73be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace22e1",
   "metadata": {},
   "source": [
    "Les quatre premières composantes principales suffisent à expliquer plus de 80% de la variance de nos données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c1895",
   "metadata": {},
   "source": [
    "La fonction suivante vous permet d'afficher le cercle des corrélations des variables. Ses paramètres principaux sont :\n",
    "* `pcs` : ndarray, les composantes principales\n",
    "* `n_comp` : int, le nombre de composantes\n",
    "* `pca` : sklearn.decomposition.PCA, l'ACP\n",
    "* `axis_ranks` : list, les indices des paires d'axes à afficher (chaque paire affichera un nouveau cercle de corrélation), exemple : [(0,1)]\n",
    "* `labels` : list, le nom des variables\n",
    "\n",
    "Appliquez la pour visualiser la projection des variables dans le premier et le second plan factoriel. Interprétez les composantes principales affichées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb139a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_circles(pcs, n_comp, pca, axis_ranks, labels=None, label_rotation=0, lims=None):\n",
    "    \"\"\"Display correlation circles, one for each factorial plane\"\"\"\n",
    "\n",
    "    # For each factorial plane\n",
    "    for d1, d2 in axis_ranks: \n",
    "        if d2 < n_comp:\n",
    "\n",
    "            # Initialise the matplotlib figure\n",
    "            fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "            # Determine the limits of the chart\n",
    "            if lims is not None :\n",
    "                xmin, xmax, ymin, ymax = lims\n",
    "            elif pcs.shape[1] < 30 :\n",
    "                xmin, xmax, ymin, ymax = -1, 1, -1, 1\n",
    "            else :\n",
    "                xmin, xmax, ymin, ymax = min(pcs[d1,:]), max(pcs[d1,:]), min(pcs[d2,:]), max(pcs[d2,:])\n",
    "\n",
    "            # Add arrows\n",
    "            # If there are more than 30 arrows, we do not display the triangle at the end\n",
    "            if pcs.shape[1] < 30 :\n",
    "                plt.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n",
    "                   pcs[d1,:], pcs[d2,:], \n",
    "                   angles='xy', scale_units='xy', scale=1, color=\"grey\")\n",
    "                # (see the doc : https://matplotlib.org/api/_as_gen/matplotlib.pyplot.quiver.html)\n",
    "            else:\n",
    "                lines = [[[0,0],[x,y]] for x,y in pcs[[d1,d2]].T]\n",
    "                ax.add_collection(LineCollection(lines, axes=ax, alpha=.1, color='black'))\n",
    "            \n",
    "            # Display variable names\n",
    "            if labels is not None:  \n",
    "                for i,(x, y) in enumerate(pcs[[d1,d2]].T):\n",
    "                    if x >= xmin and x <= xmax and y >= ymin and y <= ymax :\n",
    "                        plt.text(x, y, labels[i], fontsize='14', ha='center', va='center', rotation=label_rotation, color=\"blue\", alpha=0.5)\n",
    "            \n",
    "            # Display circle\n",
    "            circle = plt.Circle((0,0), 1, facecolor='none', edgecolor='b')\n",
    "            plt.gca().add_artist(circle)\n",
    "\n",
    "            # Define the limits of the chart\n",
    "            plt.xlim(xmin, xmax)\n",
    "            plt.ylim(ymin, ymax)\n",
    "        \n",
    "            # Display grid lines\n",
    "            plt.plot([-1, 1], [0, 0], color='grey', ls='--')\n",
    "            plt.plot([0, 0], [-1, 1], color='grey', ls='--')\n",
    "\n",
    "            # Label the axes, with the percentage of variance explained\n",
    "            plt.xlabel('PC{} ({}%)'.format(d1+1, round(100*pca.explained_variance_ratio_[d1],1)))\n",
    "            plt.ylabel('PC{} ({}%)'.format(d2+1, round(100*pca.explained_variance_ratio_[d2],1)))\n",
    "\n",
    "            plt.title(\"Correlation Circle (PC{} and PC{})\".format(d1+1, d2+1))\n",
    "            plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3458b68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pcs = pca.components_\n",
    "display_circles(pcs, 9, pca, [(0,1), (0, 2), (2, 3)], labels = features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288a999",
   "metadata": {},
   "source": [
    "Sur le premier plan factoriel, nous observons que les variables `imports` et `exports` sont liées positivement à la deuxième composante principale. Dans une moindre mesure, les variables `income` et `gdpp` sont liées positivement à la première composante principale, et les variables `child_mort` et `total_fer` négativement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb8a6e",
   "metadata": {},
   "source": [
    "Appliquez la fonction `transform` sur votre objet PCA pour appliquer l'ACP sur le dataset centré réduit et stockez le résultats dans une variable `X_pca`. Puis crééz une dataframe contenant le résultat de l'ACP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1927b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X_std)\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84f4a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(X_pca, index=df['country'])\n",
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba34314",
   "metadata": {},
   "source": [
    "Afficher la projection des individus sur les deux premiers plans factoriels. Vous pouvez utiliser un [scatter plot plotly](https://plotly.com/python/line-and-scatter/) avec la dataframe du résultat de l'ACP (pour faciliter la lecture du graphique, vous pouvez utiliser les paramètres `hover_name` ou `text` en utilisant la colonne `country` de votre dataframe d'origine).\n",
    "\n",
    "Liez ce que vous observez avec l'analyse du cercle des corrélation de ce plan factoriel. Vous pouvez faire de même pour d'autres plans factoriels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9389a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_pca, x=0, y=1, hover_name=df.country, text=df.country)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b55ccc",
   "metadata": {},
   "source": [
    "La première composante principale (en abscisse) distingue les pays riches des pays en voie de développement, ce qui s'explique par l'importance du niveau de vie et de la mortalité infantile observée sur le cercle de corrélation pour cette composante. La deuxième composante principale semble difficilement explicable. Il en est de même pour le deuxième plan factoriel :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba41ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_pca, x=2, y=3, hover_name=df.country, text=df.country)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb5d2f3",
   "metadata": {},
   "source": [
    "## 10.2 ACP sur des données synthétiques\n",
    "\n",
    "Nous allons illustrer l'effet de la normalisation et de la mise à l'échelle sur l'ACP appliqué à un dataset synthétique.\n",
    "\n",
    "Pour cela on se dote d'un dataset contenant 4 variables respectant une distribution normale et une 5ème valant 0 ou 3.\n",
    "\n",
    "La cellule suivante créé un tel dataset en utilisant numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)    # For reproducibility\n",
    "\n",
    "N = 200\n",
    "P = 5\n",
    "rho = 0.5\n",
    "\n",
    "X = np.random.normal(size=[N,P])\n",
    "X = np.append(X, 3*np.random.choice(2, size = [N,1]), axis = 1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c62d555",
   "metadata": {},
   "source": [
    "Examiner votre dataset et ses propriétés en crééant une dataframe à partir de `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d25d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic = pd.DataFrame(X)\n",
    "df_synthetic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d95b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b885d",
   "metadata": {},
   "source": [
    "Afficher la distribution des variables de ce dataset en utilisant [`pandas.DataFrame.plot.density`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.density.html) (les paramètres `sharex=True,figsize=(12,5)` rendront ce graphique plus lisible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synthetic.plot.density(sharex=True,figsize=(12,5),layout=(10,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ba633",
   "metadata": {},
   "source": [
    "Appliquer une ACP sans aucun pré-traitement. Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90948ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(2)\n",
    "low_d = pca.fit_transform(X)\n",
    "plt.scatter(low_d[:,0], low_d[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d445272",
   "metadata": {},
   "source": [
    "Appliquez une ACP après avoir réduit vos données ([`sklearn.preprocessing.normalize`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html)). Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd56dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "Xn = normalize(X)\n",
    "pca = PCA(2)\n",
    "low_d = pca.fit_transform(Xn)\n",
    "plt.scatter(low_d[:,0], low_d[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90aabd",
   "metadata": {},
   "source": [
    "Appliquez une ACP après avoir centré vos données ([`sklearn.preprocessing.scale`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html)). Que constatez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale\n",
    "Xs = scale(X)\n",
    "low_d = pca.fit_transform(Xs)\n",
    "plt.scatter(low_d[:,0], low_d[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdab340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and normalize\n",
    "Xs = StandardScaler().fit_transform(X)\n",
    "low_d = pca.fit_transform(Xs)\n",
    "plt.scatter(low_d[:,0], low_d[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab3e6a2",
   "metadata": {},
   "source": [
    "Sans normalisation, deux groupes de points distincts semblent visibles sur la projection dans le premier plan factoriel. Avc normalisation, nous n'observons plus cette répartition, ce qui est conforme à une ACP sur des données aléatoires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e750e",
   "metadata": {},
   "source": [
    "## 10.3 Appliquez une ACP sur des variations de cours d'actions\n",
    "\n",
    "Appliquez une ACP sur le dataset contenu dans le fichier `data/company-stock-movements-2010-2015-incl.csv` qui contient les variations quotidiennes des valeurs de titres boursiers en fin de séance par rapport à la valeur de la veille de 2010 à 2015.\n",
    "\n",
    "Note : précisez à Pandas lors de la lecture du fichier CSV que la première colonne contient l'index (paramètre `index_col=0`).\n",
    "\n",
    "Pour visualiser ces variations pour un ou plusieurs titres vous pouvez utiliser le code suivant (une fois votre dataframe chargée) : `df.loc[['Goldman Sachs', 'Amazon']].transpose().plot(figsize=(15, 7))`.\n",
    "\n",
    "Attention : l'analyse des cercles de corrélations n'est pas utile pour cette exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64079f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock = pd.read_csv('/data/company-stock-movements-2010-2015-incl.csv', index_col=0)\n",
    "print(df.shape)\n",
    "df_stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914ce8f",
   "metadata": {},
   "source": [
    "Vérifions si ce dataset contient des valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(df_stock).any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a966e302",
   "metadata": {},
   "source": [
    "### 10.3.1 Visualisation des mouvements\n",
    "\n",
    "Nous pouvons visualiser les mouvements d'un ou plusieurs titres sur un graphique. Le résultat semble peu exploitable pour identifier des titres ayant des variations de valeurs proches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231311ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock.loc[['Goldman Sachs', 'Amazon']].transpose().plot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b456f8",
   "metadata": {},
   "source": [
    "### 10.3.2 Analyse en Composantes Principales\n",
    "\n",
    "Nous allons réduire le nombre de dimensions à l'aide d'une ACP pour projeter les titres sur le plan des deux premières composantes principales. Appliquons une ACP en précisant que l'on souhaite obtenir suffisament de composantes pour expliquer 95% de la variance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "# remplacer par X = df.values pour une ACP sur les données non normalisées\n",
    "X = scaler.fit_transform(df_stock.values)\n",
    "X = normalize(df_stock.values)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aec91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c88f15",
   "metadata": {},
   "source": [
    "Les deux premières composantes principales n'expliquent que 13% de la variance de nos données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1826e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e762bc",
   "metadata": {},
   "source": [
    "Nombre de composantes nécessaires pour atteindre 95% de la variance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa058e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031fe495",
   "metadata": {},
   "source": [
    "### 10.3.3 Projection sur le premier plan factoriel\n",
    "\n",
    "Visualisons les titres projetés sur le plan des deux premières composantes principales. Pour cela nous construisons une dataframe contenant le résultat de notre ACP puis nous affichons le graphique correspondant à cette projection :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c996cd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stock_pca = pd.DataFrame(X_pca, index=df_stock.index)\n",
    "df_stock_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819c0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df_stock_pca, x=0, y=1, hover_name=df_stock.index, text=df_stock.index)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc264178",
   "metadata": {},
   "source": [
    "Plusieurs groupes de titres sont identifiables sur cette représentation tel qu'un groupe composé des titres technologiques (Yahoo, Amazon, Google ...) et un autre composé des titres des majors pétrolières (Total, Chevron, Shell ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d87e2",
   "metadata": {},
   "source": [
    "### 10.3.4 Visualisation de quelques titres proches\n",
    "\n",
    "Pour visualiser les variations de certains titres, nous allons créer une dataframe contenant les données normalisées (nous nous intéressons aux variations relatives et non aux variations absolues).\n",
    "\n",
    "Mais la visualisation des données brutes de plusieurs titres proches dans notre espace réduit à l'aide d'une ACP ne permet pas d'apprécier leurs comportements similaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e247f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_normalized = pd.DataFrame(X, columns=df_stock.columns, index=df_stock.index)\n",
    "df_stock_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d288a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_normalized.loc[['Goldman Sachs', 'Bank of America', 'Microsoft', 'HP']].transpose().plot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd1283",
   "metadata": {},
   "source": [
    "Mais nous pouvons lisser ces courbes à l'aide de la méthode [pandas.DataFrame.rolling](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html) de pandas. Le résultat est plus satisfaisant et permet d'observer des similarités dans les variations de la valeur des titres (nous présentons ici plusieurs comparaisons pertinentes) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_normalized.loc[['Goldman Sachs', 'Bank of America', 'Microsoft', 'HP']] \\\n",
    "    .transpose().rolling(window=200, center=True).mean().plot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40112545",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_normalized.loc[['Goldman Sachs', 'Bank of America', 'Colgate-Palmolive', 'Kimberly-Clark']] \\\n",
    "    .transpose().rolling(window=200, center=True).mean().plot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e28dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_normalized.loc[['Goldman Sachs', 'Bank of America', 'Total', 'Chevron']] \\\n",
    "    .transpose().rolling(window=200, center=True).mean().plot(figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd7b7a",
   "metadata": {},
   "source": [
    "**Bonus : Clustering hierarchique**\n",
    "\n",
    "Une méthode de clustering devrait permettre d'obtenir des groupes de titres similaires. Appliquons un clustering hiérachique en utilisant la librairie scipy qui permet d'obtenir une visualisation du dendrogramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "linked = linkage(df_stock_normalized, 'ward')\n",
    "\n",
    "labels = df_stock_normalized.index\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            labels=labels,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True,\n",
    "            leaf_font_size=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb60d5",
   "metadata": {},
   "source": [
    "Scikit-learn fournit une classe permettant d'appliquer un clustering hiérarchique en précisant le nombre de clusters que nous souhaitons obtenir :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "cluster = AgglomerativeClustering(n_clusters=9, affinity='euclidean', linkage='ward')  \n",
    "labels = cluster.fit_predict(df_stock_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815fd81",
   "metadata": {},
   "source": [
    "Finalement, nous pouvons ajouter les labels des clusters pour chaque titre dans notre dataframe d'origine, puis afficher le label pour chaque titre en ordonnant le résultat par le numéro de label :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993b1ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_stock['label'] = labels\n",
    "df_stock['label'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beab0722",
   "metadata": {},
   "source": [
    "Nous observons que cette méthode permet d'identifier des clusters de titres comprenant des entreprises dont l'activité est proche. Par exemple, le cluster 4 comprend des entreprises de la défense (Boeing, Lookheed Martin et Northrop Grumman). Mais certains clusters semblent peu cohérents, comme le cluster 1 qui contient aussi bien Apple que McDonalds. Il serait intéressant de tester des variations sur le nombre de clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d8425",
   "metadata": {},
   "source": [
    "## 10.4 Compréssion d'image\n",
    "\n",
    "L'ACP peut aussi être utile pour compresser des données. Nous allons appliquer cette méthode pour compréser une image. La cellule suivante charge les modules nécessaires, charge et affiche l'image de test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db8d554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from matplotlib import image\n",
    "\n",
    "img = image.imread('/data/bird.png')\n",
    "print(img.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ca6b5",
   "metadata": {},
   "source": [
    "L'image contient 256 lignes, 349 colonnes et 4 canaux : un par couleur (RGB) et un pour le canal alpha (la transparence, une spécificité du format PNG). Nous commencons par transformer notre image pour obtenir un tableau de 256 lignes et 1396 colonnes (les 4 composantes sont simplement concaténées) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4511ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "img_reshaped = np.reshape(img, (img.shape[0], img.shape[1] * img.shape[2]))\n",
    "img_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb6f19",
   "metadata": {},
   "source": [
    "Puis nous appliquons notre ACP en précisant le nombre de composantes que nous souhaitons obtenir, dans notre exemple, notre image compressée contiendra donc 256 lignes et 32 colonnes (nos 32 composantes principales) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(32)\n",
    "img_compressed = pca.fit_transform(img_reshaped)\n",
    "img_compressed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3639d",
   "metadata": {},
   "source": [
    "Nous avons donc un taux de compression d'environ 43 (notre image compressée est 43 fois plus légère que notre image d'origine, attention : il faudrait aussi compter le poids des coefficients de nos composantes principales pour être exact) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9eadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size, img_compressed.size, img.size / img_compressed.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72a2c5",
   "metadata": {},
   "source": [
    "Nos premières composantes principales expliquent une grande part de la variance de notre image, nos 32 composantes expliquent plus de 98% de sa variance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84727aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb076adb",
   "metadata": {},
   "source": [
    "Finalement, nous pouvons utiliser la méthode `inverse_transform` de la classe `PCA` de scikit-learn pour retrouver les données projetées dans le même espace dimensionnel que l'image source (i.e. : 1396 colonnes, onc 349 colonnes sur 4 canaux) et afficher cette image :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35182747",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_reconstructed = pca.inverse_transform(img_compressed)\n",
    "print(image_reconstructed.shape)\n",
    "# reshaping 1396 back to the original 349 * 4\n",
    "image_reconstructed = np.reshape(image_reconstructed, (img.shape[0],img.shape[1],img.shape[2]))\n",
    "print(image_reconstructed.shape)\n",
    "plt.axis('off')\n",
    "plt.imshow(image_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17437892",
   "metadata": {},
   "source": [
    "La cellule suivante affiche un slider permettant de choisir le nombre de composantes que l'on souhaite conserver et affiche l'image d'origine et l'image compressée côte à côte :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db9e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Layout, interact\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def compress_image(n_components=32):\n",
    "    pca = PCA(n_components)\n",
    "    img_compressed = pca.fit_transform(img_reshaped)\n",
    "    image_reconstructed = pca.inverse_transform(img_compressed)\n",
    "    image_reconstructed = np.reshape(image_reconstructed, (img.shape[0],img.shape[1],img.shape[2]))\n",
    "    f, axarr = plt.subplots(1,2, figsize=(20,10))\n",
    "    axarr[0].imshow(img)\n",
    "    axarr[0].axis('off')\n",
    "    axarr[1].imshow(image_reconstructed)\n",
    "    plt.axis('off')\n",
    "\n",
    "interact(\n",
    "    compress_image,\n",
    "    n_components=widgets.IntSlider(\n",
    "        min=1, max=256, step=1, value=32,\n",
    "        layout=Layout(width='500px'),\n",
    "        continuous_update = False\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cffc5af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
