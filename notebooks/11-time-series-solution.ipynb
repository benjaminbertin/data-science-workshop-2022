{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YT2Czxw-t_6p"
   },
   "source": [
    "# 11. Manipulation de séries temporelles\n",
    "\n",
    "Ce notebook permet de découvrir la manipulation des données temporelles avec l'écosystème pandas. Nous découvrirons quelques problématiques courantes lors de la manipulation de données temporelles (changement d'échelle de dates, enrichissement de données, sélection de données, ...).\n",
    "\n",
    "Le dataset utilisé correspond au trafic automobile sur une route comportant plusieurs voies de circulation. Les trafics des différentes voies ont été agrégés en une valeur `flow`. L'attribut `velocity` indique la vitesse moyenne des véhicules sur toutes les voies. Deux colonnes indiquent si la mesure a été prise pendant un jour férié ou pendant une période de vacance scolaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "4WpDYRqLrCzg",
    "outputId": "9c28cd19-f3bb-43e2-f73f-d919d0d8b274"
   },
   "outputs": [],
   "source": [
    "!pip install plotly==5.3.1\n",
    "import plotly\n",
    "\n",
    "plotly.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "lbATG16arD1t",
    "outputId": "5bb73e08-197d-4987-cd54-b7f71c0edbdd"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "#py.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_c2MtCu_tmlk"
   },
   "source": [
    "## 11.1 Chargement du dataset\n",
    "\n",
    "Chargez le fichier `data/traffic-data.csv` dans une dataframe, puis :\n",
    "* Affichez les types des colonnes du dataset\n",
    "* Visualisez les 20 premières lignes et un échantillon pour intépréter les colonnes non numériques\n",
    "* Affichez une description statistique des variables numériques\n",
    "* Affichez un histogramme et un boxplot pour chaque grandeur numérique\n",
    "* Affichez la matrice de corrélation entre ces grandeurs numériques\n",
    "* Testez la significativité de la corrélation de pearson entre `flow` et `velocity` avec [`scipy.stats.pearsonr`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "KoJX02R3t4PR",
    "outputId": "c2a57515-48d9-4aee-e4c1-0bd373cbadbb"
   },
   "outputs": [],
   "source": [
    "filename = '/data/traffic-data.csv'\n",
    "data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YkDEbPMv33o"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKoIYRHziYtx"
   },
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrteDeKU12RN"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJcilRD816lZ"
   },
   "outputs": [],
   "source": [
    "data[['flow', 'occupation', 'velocity']].hist(figsize=(20, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKjM3kmj3wBI"
   },
   "outputs": [],
   "source": [
    "data.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2PM-NK24RSv"
   },
   "outputs": [],
   "source": [
    "data[['flow','occupation', 'velocity']].corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzHkcWcfzsz-"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr(data.flow, data.velocity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM_Ew6c3zh1e"
   },
   "source": [
    "## 11.2 Manipulation des dates\n",
    "\n",
    "Il existe de nombreux formats pour les valeurs de temps (timestamps) : Timestamps numpy et datetime en particulier. Jouons un peu avec ces formats :\n",
    "\n",
    "A l'aide du module [`time`](https://docs.python.org/3/library/time.html), définissez une variable `current_time` calculée au moment de l'exécution. Puis utilisez la méthode [`strftime`](https://docs.python.org/3/library/datetime.html#datetime.date.strftime) pour la convertir sous la forme d'une chaîne de caractère au format `YYYY-MM-DD HH:mm:ss` et stockez cette chaine dans une variable :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPG8cgSp0lKZ"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "current_time = time.localtime()\n",
    "str_current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\", current_time)\n",
    "print(str_current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1epaM0W61wW"
   },
   "source": [
    "Convertissez cette chaîne de caractère au format `np.datetime64` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZr9cIom5Fh0"
   },
   "outputs": [],
   "source": [
    "np_current_time = np.datetime64(str_current_time)\n",
    "np_current_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUsp0k2fSAAt"
   },
   "source": [
    "Utilisez la méthode [`datetime.datetime.strptime`](https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime) du package `datetime` pour parser la date qui est sous forme de chaîne de caractères en datetime :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPUfPQZkSAM_"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "dt.strptime(str_current_time, \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1l5KRNVhSQJl"
   },
   "source": [
    "Transformez la date depuis le format numpy vers le format datetime :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KV5Vcs5SQSy"
   },
   "outputs": [],
   "source": [
    "dt.strptime(str(np_current_time), \"%Y-%m-%dT%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeBeQKhF0G_R"
   },
   "source": [
    "Le format de données [`datetime`](https://docs.python.org/3/library/datetime.html#datetime-objects) est le plus riche : il offre le plus de fonctionnalités, comme l'accès à certains attributs très utiles pour les dates : année, mois, semaine dans le calendrier annuel, jour de la semaine... Pour pouvoir en tirer parti, il faut convertir nos horodates en datetime. Convertissez la colonne `horodate` en datetime en utilisant la fonction [`pandas.to_datetime`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00cXJ9ZqlCNs"
   },
   "outputs": [],
   "source": [
    "data['horodate'] = pd.to_datetime(data.horodate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9B3-q3nO0gzn"
   },
   "source": [
    "Ajoutez à data une colonne pour :\n",
    "- l'année\n",
    "- le mois\n",
    "- la date (YYYY-MM-DD)\n",
    "- le jour de la semaine\n",
    "- l'heure\n",
    "- la minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ak_bHWWUfT6"
   },
   "outputs": [],
   "source": [
    "data['year'] = data['horodate'].dt.year\n",
    "data['month'] = data['horodate'].dt.month\n",
    "data['date'] = data['horodate'].dt.date\n",
    "data['weekday'] = data['horodate'].dt.weekday\n",
    "data['hour'] = data['horodate'].dt.hour\n",
    "data['minute'] = data['horodate'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcMXqFdehOK1"
   },
   "source": [
    "## 11.3 Vérification de la complétion temporelle de la série"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooiJxGqkWXTX"
   },
   "source": [
    "Une série temporelle possède une fréquence $F$ : toutes les $F$ secondes, une valeur doit être présente dans les données.\n",
    "Il peut arriver qu'une série temporelle soit incomplète. Deux cas peuvent expliquer cela :\n",
    "1. Absence d'enregistrement à une horodate donnée donnant lieu à un tuple manquant.\n",
    "2. Absence de données due au changement d'horaire. En effet, les séries temporelles sont souvent exprimées sur l'échelle de temps UTC, qui est une échelle universelle. Les échelles de temps locales sont généralement décalées par rapport à UTC et la valeur de ce décalage change avec le changement d'heure hiverna/estival. \n",
    "\n",
    "Or pour certains usages (ex. pour la prédiction de trafic), les régularités de trafic s'observent non pas en UTC, mais en temps local.\n",
    "\n",
    "Il est donc nécessaire de convertir des données UTC en localtime, ce qui genère des trous et des redondances dans les données..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsEf6hhlWLJ5"
   },
   "source": [
    "Trouvez les dates de début et de fin du dataset et calculez l'extension temporelle correspondante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffFagKg5XTfA"
   },
   "outputs": [],
   "source": [
    "start = data.horodate.min()\n",
    "end = data.horodate.max()\n",
    "\n",
    "timerange = end -start \n",
    "timerange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour convertir les datetime UTC en dates locales (CET), nous pouvons utiliser Pandas ou la librairire pytz. Une datetime est par défaut sans timezone (timezone naive). Il faut donc dans un premier temps convertir notre datetime en timezone aware puis la convertir dans la timezone cible.\n",
    "\n",
    "Notez la présence de la timezone dans l'affichage de la nouvelle datetime et la conversion en temps local dans la sortie du formattage de cette datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "mydt = dt(2018, 5, 25, 15, 12, 30)\n",
    "cet_tz = pytz.timezone('CET')\n",
    "# pytz.fromutc(mydt, is_dst=None)\n",
    "mydt_utc = pytz.utc.localize(mydt)\n",
    "print(mydt_utc)\n",
    "mydt_cet = mydt_utc.astimezone(cet_tz)\n",
    "print(mydt_cet)\n",
    "print(mydt.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "cet_tz.utcoffset(mydt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une fonction `convert_utc_to_cet` prenant en paramètre une datetime et renvoyant cette datetime dans la timezone CET, puis créez une nouvelle colonne `localtime_horodate` dans votre dataframe en utilisant la méthode [`pandas.Series.apply`](https://pandas.pydata.org/docs/reference/api/pandas.Series.apply.html) sur la colonne `horodate` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHyRZ9lModPD"
   },
   "outputs": [],
   "source": [
    "def convert_utc_to_localtime(date):\n",
    "    cet_tz = pytz.timezone('CET')\n",
    "    return pytz.utc.localize(date).astimezone(cet_tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ovim17ldswjh"
   },
   "outputs": [],
   "source": [
    "data['localtime_horodate'] = data.horodate.apply(convert_utc_to_localtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJbwWtxJdVLZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est-ce que cette méthode prend en compte les changements d'heures d'été/d'hiver ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[(data.localtime_horodate >= '2018-03-25') & (data.localtime_horodate <= '2018-03-26')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg86sy5caAuw"
   },
   "source": [
    "Pour rechercher les horodates manquantes dans la série temporelle indexée sur le temps local, nous pouvons générer la liste complète des horodates à la fréquence de 6 minutes entre le premier et le dernier horodate en temps local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7yp_mkF7fyF"
   },
   "source": [
    "Utilisez [`pandas.date_range`](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html) pour obtenir la liste des timestamps sur la plage de temps considérée, puis créez une dataframe contenant une seule colonne avec cette liste :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEdheLiCdQab"
   },
   "outputs": [],
   "source": [
    "complete_horodates = pd.date_range(convert_utc_to_localtime(start), convert_utc_to_localtime(end), freq='6T')\n",
    "complete_tr = pd.DataFrame(data={'localtime_horodate_complete': complete_horodates})\n",
    "complete_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGCPUn51hDmk"
   },
   "source": [
    "Réalisez une jointure avec [`pandas.merge`](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) entre cette dataframe et notre dataframe `data` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoY9Xe3v7qmE"
   },
   "outputs": [],
   "source": [
    "complete_tr = pd.DataFrame(data={'localtime_horodate_complete': complete_horodates})\n",
    "data = data.merge(complete_tr,\n",
    "                  left_on='localtime_horodate', \n",
    "                  right_on='localtime_horodate_complete', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_zf78Yg9Yph"
   },
   "source": [
    "Identifiez les tuples correspondant à des horodates manquantes dans le dataset et déterminez la cause probable pour ces manquants :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBwz4zn87rZ1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_with_missing_values = data.loc[data.horodate.isna()].index\n",
    "\n",
    "data.iloc[rows_with_missing_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce sont probablement des défauts de capteur ou de remonté de donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUfMlmBVz9dt"
   },
   "source": [
    "## 1.3 Correction des valeurs (valeurs décalées et manquantes)\n",
    "\n",
    "Nous allons remplacer ces valeurs manquantes par la valeur pour le même jour la semaine suivante. Nous pourrions aussi recourir à la méthode `interpolate` pour réaliser une interpolation des valeurs manquantes entre deux valeurs présentes.\n",
    "\n",
    "Commencez par appliquez la méthode [`pandas.DataFrame.shift`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html) sur les colonnes `flow` et `velocity` pour obtenir un décalage d'une semaine :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['flow'].shift(10 * 24 * 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez la méthode [`pandas.DataFrame.fillna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html) pour remplacer les valeurs manquantes en utilisant les valeurs de la semaine suivante pour les colonnes `flow` et `velocity` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['flow'] = data.flow.fillna(data.flow.shift(10 * 24 * 7))\n",
    "data['velocity'] = data.velocity.fillna(data.flow.shift(10 * 24 * 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez les valeurs que nous venont d'imputer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[rows_with_missing_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recréez les colonnes indiquant l'année, le mois, la date (YYYY-MM-DD), le jour de la semaine, l'heure, la minute depuis la colonnes `localtime_horodate_complete` pour obtenir ces valeurs en timezone CET et quelles soient présentes pour toutes les lignes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex7tV5MBjdCG"
   },
   "outputs": [],
   "source": [
    "data['year'] = data['localtime_horodate_complete'].dt.year\n",
    "data['month'] = data['localtime_horodate_complete'].dt.month\n",
    "data['date'] = data['localtime_horodate_complete'].dt.date\n",
    "data['weekday'] = data['localtime_horodate_complete'].dt.weekday\n",
    "data['hour'] = data['localtime_horodate_complete'].dt.hour\n",
    "data['minute'] = data['localtime_horodate_complete'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[rows_with_missing_values]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "g_WoJBSoz34n"
   },
   "source": [
    "# Alternative\n",
    "\n",
    "import math \n",
    "\n",
    "numerical_columns = data.select_dtypes(include=[np.number]).columns \n",
    "\n",
    "for column in numerical_columns:\n",
    "    data['{}_imputation_value'.format(column)] = data[column].shift(10 * 24 * 7)\n",
    "    data[column] = np.vectorize(lambda val, imput_val:\n",
    "                                imput_val if math.isnan(val) else val)(\n",
    "                                  data[column],\n",
    "                                  data['{}_imputation_value'.format(column)]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysez le code suivant, décrivez son fonctionnement et le résultat obtenu (pensez à utiliser des affichages intermédiaires des différentes étapes) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG7DS7xUhEFg"
   },
   "outputs": [],
   "source": [
    "data_mode_per_day = data[['date', 'bank_holiday', 'school_holiday']].groupby('date').agg(pd.Series.mode).reset_index()\n",
    "data_mode_per_day.date = data_mode_per_day.date.astype(str)\n",
    "data.date = data.date.astype(str)\n",
    "data = data.merge(\n",
    "    data_mode_per_day,\n",
    "    on='date',\n",
    "    suffixes=('', '_imputation_value'),\n",
    "    how='left'\n",
    ")\n",
    "for column in ['bank_holiday', 'school_holiday']:\n",
    "    data[column] = data['{}_imputation_value'.format(column)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mode_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[rows_with_missing_values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCVDm8x3fJf6"
   },
   "source": [
    "Quelles sont les limites de la méthode d'imputation que vous avez appliqué ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode ne fonctionnera s'il manque plusieurs jours consécutifs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimez les colonnes inutiles (`horodate`, `bank_holiday_imputation_value` et `school_holiday_imputation_value`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlChwRquiESx"
   },
   "outputs": [],
   "source": [
    "data = data[[\n",
    "    'flow', 'occupation', 'velocity', 'date', 'bank_holiday', 'school_holiday',\n",
    "    'year', 'month', 'weekday', 'hour', 'minute', 'localtime_horodate', 'localtime_horodate_complete'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UItrXUGZ-FwI"
   },
   "source": [
    "## 11.4 Identification des dates en doublon\n",
    "\n",
    "Y a-t-il des des doublons sur l'horodate local ? S'il y en a, supprimez les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pA0VZ4mAWB00"
   },
   "outputs": [],
   "source": [
    "data[data.duplicated(subset=['localtime_horodate_complete'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj6-PfgEnlkR"
   },
   "source": [
    "## 11.5 Manipulation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNJZtxKUoD_l"
   },
   "source": [
    "Pour effectuer des analyses temporelles sur les données, il est souvent pratique d'indéxer la dataframe sur les horodates. Ré-indexez votre dataframe avec la colonne de l'horodate locale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ryie6qSVn1zX"
   },
   "outputs": [],
   "source": [
    "data.set_index(\"localtime_horodate_complete\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV_V_q24_m1X"
   },
   "source": [
    "### 11.5.1 Analyse de la saisonnalité des données\n",
    "\n",
    "Utilisez la fonction [`pandas.plotting.autocorrelation_plot`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html) pour afficher le diagramme d'autocorrélation du trafic et/ou utilisez la fonction `plot_autocorrelation_lags` définie ci-dessous. Puis expliquez ce diagramme d'autocorrélation, les données sont-elles saisonnières ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(data.flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ldD5kOH_qug"
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def plot_autocorrelation_lags(s):\n",
    "    corr = signal.correlate(s - s.mean(), s - s.mean())\n",
    "    corr /= np.max(corr)\n",
    "    corr = corr[len(s):]\n",
    "\n",
    "    fig = px.scatter(corr)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4xdOQEO_q2k"
   },
   "outputs": [],
   "source": [
    "plot_autocorrelation_lags(data.flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données présentent une double saisonnalité : hebdomadaire et journalière."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duO7xdidcRGw"
   },
   "source": [
    "### 11.5.2 Rééchantillonnage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-SZO6AenB31"
   },
   "source": [
    "Avec pandas, certaines opérations sur les séries temporelles sont très simplifiées, comme le rééchantillonnage.\n",
    "\n",
    "Rééchantillonnez le dataset avec une fréquence de 12min (créez une dataframe `data_12`) et 24 min (`data_24`) en utilisant la méthode [`pandas.DataFrame.resample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html), vous devez choisir une méthode d'aggrégation adaptée (pour les colonnes flow, occupation et velocity uniquement) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcPpgdHj92Mq"
   },
   "outputs": [],
   "source": [
    "data_12 = data.resample('12T').agg(\n",
    "    {\n",
    "        'flow': np.sum,\n",
    "        'occupation': lambda x: pd.Series.mode(x)[0],\n",
    "        'velocity': np.mean\n",
    "    }\n",
    ")\n",
    "data_24 = data.resample('24T').agg(\n",
    "    {\n",
    "        'flow': np.sum,\n",
    "        'occupation': lambda x: pd.Series.mode(x)[0],\n",
    "        'velocity': np.mean\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsMTWnbgpGXF"
   },
   "source": [
    "Affichez les données de trafic (`flow`) avec la fonction [`line`](https://plotly.com/python/line-charts/)plotly express pour les 3 premiers jours de `data`, `data_12` et `data_24`. Que remarquez-vous quant à l'effet du resampling ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qpcd1gtspeUW"
   },
   "outputs": [],
   "source": [
    "fig = px.line(data[data.index < '2017-11-08'], y='flow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(data_12[data_12.index < '2017-11-08'], y='flow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(data_24[data_24.index < '2017-11-08'], y='flow')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données rééchantillonnées sont moins bruitées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w91SQub_qY2v"
   },
   "source": [
    "### 11.5.3 Simplification des données\n",
    "\n",
    "Simplifions les données en ne retenant que le trafic pour les mardis. Créez une dataframe `data_tuesday` ne contenant que les mardis, affichez l'autocorrélogramme du trafic et comparez le résultat obtenu avec celui obtenu sur tout le dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqAg-zlkkkPE"
   },
   "outputs": [],
   "source": [
    "data_tuesday = data.loc[data.weekday == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3YdFlDmCCFk",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_autocorrelation_lags(data_tuesday.flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oT1rE5ssBYK"
   },
   "source": [
    "Affichez les courbes de trafic pour tous les mardis de notre dataset avec `px.line`. Pour obtenir le meilleur affichage possible :\n",
    "* appelez la méthode `update_xaxes` sur votre figure, après l'avoir créé et avant de l'afficher, en utilisant le paremètre `rangebreaks=[{'bounds': [\"wed\", \"tues\"]}]` pour cacher les sauts de dates\n",
    "* utiliser le paramètre `render_mode=\"svg\"` pour l'appel à `px.line` (pour éviter un bug connu de plotly sur l'affichage d'un graphique avec un saut de dates et le moteur de rendu WebGL)\n",
    "\n",
    "Remarquez-vous des particularités ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToCIHiA5nepL"
   },
   "outputs": [],
   "source": [
    "fig = px.line(data_tuesday, y='flow', render_mode=\"svg\")\n",
    "\n",
    "fig.update_xaxes(rangebreaks=[{'bounds': [\"wed\", \"tues\"]}])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgyV1Yu-wfxM"
   },
   "source": [
    "### 11.5.4 Détection des outliers\n",
    "\n",
    "On considère mettre en œuvre un modèle prédictif linéaire pour les données de trafic du dataset. D'après le graphique ci-dessus, les données ne sont pas très qualitatives...\n",
    "\n",
    "**Identifiez deux problèmes dans ces données.**\n",
    "\n",
    "L'unité de détection des anomalies doit respecter la saisonnalité, soit dans cette série simplifiée, la journée.\n",
    "\n",
    "Pour détecter (et éliminer) les jours où les données sont de mauvaise qualité, nous proposons d'utiliser la décomposition . L'idée est la suivante :\n",
    "- On effectue une décomposition \n",
    "- Les valeurs de tendance, de saisonnalité et de résidu à chaque horodate peuvent être considérées comme des attributs\n",
    "- On cherche les anomalies dans l'espace de ces attributs au moyen d'un algorithme d'identification d'outliers, par ex. IsolationTrees\n",
    "- On devra alors rejeter les jours contenant trop d'horodates 'outlier' selon cet algorithme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.5.4.1 Décomposition\n",
    "\n",
    "Réalisez une décomposition du trafic avec la fonction [`statsmodels.tsa.seasonal.seasonal_decompose`](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) avec les paramètres suivants :\n",
    "* `x=df.flow.values` : essayez de passer directement `df.flow` puis commentez)\n",
    "* `model='additive'` : comparez `additive` et `multiplicative`\n",
    "* `period: int` : trouvez la bonne valeur\n",
    "* `extrapolate_trend=True` : pour obtenir une trend sur toute la série observée\n",
    "\n",
    "Puis affichez les valeurs des attributs résultant de la décomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "statsmodels.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "df = data_tuesday.copy()\n",
    "\n",
    "st_result = seasonal_decompose(\n",
    "    df.flow.values,\n",
    "    model=\"additive\",\n",
    "    period=24*10,\n",
    "    extrapolate_trend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_result.observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez les graphiques de cette décomposition en utilisant la méthode `plot` sur le résultat de la décomposition. Vous pouvez afficher un graphique plus lisible en utilisant les instructions `fig.set_size_inches((16, 9))` et `fig.tight_layout()` (prenez soin d'affecter le résultat de l'appel à `plot` dans une variable nommée `fig`) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = st_result.plot()\n",
    "fig.set_size_inches((16, 9))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "n-XIM-Au3vN3"
   },
   "source": [
    "season = ''\n",
    "df = data_tuesday.copy()\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "st = seasonal_decompose(df.flow.values,\n",
    "                         model=\"additive\",\n",
    "                         period=24*10,\n",
    "                         extrapolate_trend=True\n",
    "                         )\n",
    "\n",
    "st_dataframe = pd.DataFrame(index=df.index.astype('category'))\n",
    "\n",
    "st_dataframe['observed'] = st.observed\n",
    "st_dataframe['seasonal'] = st.seasonal\n",
    "st_dataframe['trend'] = st.trend\n",
    "st_dataframe['resid'] = st.resid\n",
    "\n",
    "\n",
    "st_dataframe['observed'].plot(figsize=(20, 3))\n",
    "plt.show()\n",
    "st_dataframe['seasonal'].plot(figsize=(20, 3))\n",
    "plt.show()\n",
    "st_dataframe['trend'].plot(figsize=(20, 3))\n",
    "plt.show()\n",
    "st_dataframe['resid'].plot(figsize=(20, 3))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez des colonnes dans votre dataframe contenant les valeurs de saisonnalité, de tendance et de résidu de notre décomposition pour chaque ligne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['seasonal'] = st_result.seasonal\n",
    "df['trend'] = st_result.trend\n",
    "df['resid'] = st_result.resid\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.5.4.2 Recherche des outliers\n",
    "\n",
    "Entrainez un modèle [`sklearn.ensemble.IsolationForest`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html) sur la résidu de la décomposition en précisant comme paramètre lors de la création de l'instance `contamination=.2`. Puis réalisez une prédiction sur sur ce même résidu, interprétez le résultat et affectez le à une colonne `valid_value` de votre dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = IsolationForest(contamination=.2)\n",
    "df['valid_value'] = model.fit_predict(df[['resid']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.valid_value.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une series contenant la somme des `valid_value` par jour puis ajoutez une colonne (nommée `valid_value_sum`) indiquant la valeur de cette somme pour chaque ligne (indices : `groupby` et `merge` permettent de réaliser cette opération) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.merge(\n",
    "    df.groupby('date').sum().valid_value,\n",
    "    right_index=True,\n",
    "    left_on='date',\n",
    "    suffixes=('', '_sum')\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez un graphique des `valid_value_sum` et en dessous un graphique de la colonne `flow`, que constatez-vous ? (pour plus de lisibilité, vous pouvez réalisez une copie de votre dataframe et remplacer l'index par `df.index.astype('category')`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df.copy()\n",
    "df_plot.index = df_plot.index.astype('category')\n",
    "df_plot.valid_value_sum.plot(figsize=(30, 5))\n",
    "plt.show()\n",
    "df_plot.flow.plot(figsize=(30, 5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut se fixer un seuil grossier pour décider quand une date doit être considérée comme outlier. Ajoutez une colonne `date_is_outlier` qui vaut `True` si `valid_value_sum` est inférieure à 100 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K7-o1Y2Qlcl"
   },
   "outputs": [],
   "source": [
    "df['date_is_outlier'] = df.valid_value_sum < 100\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quels sont les jours outliers selon notre critère ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.date_is_outlier].date.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une nouvelle dataframe contenant que le trafic des jours qui ne sont pas outliers puis affichez un graphique des valeurs de `flow` pour cette dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_data = df.loc[df.date_is_outlier == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DpgDAa2HJCK"
   },
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    clean_data,\n",
    "    x=clean_data.index,\n",
    "    y='flow',\n",
    "    render_mode='svg'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(\n",
    "    rangebreaks=[\n",
    "        dict(bounds=[\"wed\", \"tues\"]),\n",
    "    ]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCK8fx-32v3e"
   },
   "source": [
    "Vérifiez que le nettoyage donne une décomposition plus encourageante..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_result = seasonal_decompose(\n",
    "    clean_data.flow.values,\n",
    "    model=\"additive\",\n",
    "    period=24*10,\n",
    "    extrapolate_trend=True\n",
    ")\n",
    "\n",
    "fig = st_result.plot()\n",
    "fig.set_size_inches((16, 9))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4t9D8bvx6Z1"
   },
   "source": [
    "Le nettoyage vous paraît-il satisfaisant en vue d'une prédiction par des modèles linéaires (moyenne glissante, sarima, etc.) ? Proposez (et testez !) des modifications simples."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_exploration_and_preparation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
