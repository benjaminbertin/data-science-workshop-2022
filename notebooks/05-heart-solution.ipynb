{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prédire les maladies cardiaques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous travaillons sur un jeu de données fourni par le projet __[drivendata](https://www.drivendata.org)__ : __[Warm Up: Machine Learning with a Heart](https://www.drivendata.org/competitions/54/machine-learning-with-a-heart/)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data/heart_values.csv')\n",
    "df_label = pd.read_csv('/data/heart_labels.csv')\n",
    "df = pd.merge(df, df_label, on='patient_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Analyse exploratoire des données\n",
    "\n",
    "Explorez ce dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des corrélations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(df.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modalités des variables catégorielles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=[object]).columns:\n",
    "    counts = df[col].value_counts()\n",
    "    if len(counts) < 20:\n",
    "        print(df[col].value_counts(), \"\\n\")\n",
    "    else:\n",
    "        print(\"{} has {} unique values\\n\".format(col, len(counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.plot.kde(\n",
    "    subplots=True,\n",
    "    layout=(5, 3),\n",
    "    legend=False,\n",
    "    title=df.dtypes[df.dtypes != 'object'].index.tolist(),\n",
    "    figsize=(20, 15)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.dtypes[df.dtypes != 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution des valeurs pour chaque variable numérique :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(20, 15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Classement des variables en fonction de leurs types\n",
    "\n",
    "Ce dataset contient des variables de plusieurs types (numériques, catégorielles et ordinales). Nous devons appliquer des pre-traitements différents pour chaque type de variable.\n",
    "\n",
    "Identifiez les types pour chaque variable et construisez trois listes Python contenant le nom des variables pour trois types : `var_names_num` (type numérique) `var_names_cat` (type catégoriel), `var_names_ord` (type ordinal). La variable `age` sera traitée différement (i.e. : ne l'incluez dans aucune de ces listes) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(df)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names_num = ['max_heart_rate_achieved', 'oldpeak_eq_st_depression', \n",
    "                 'resting_blood_pressure', 'serum_cholesterol_mg_per_dl']\n",
    "var_names_cat = ['thal', 'exercise_induced_angina', 'sex',\n",
    "                 'fasting_blood_sugar_gt_120_mg_per_dl',\n",
    "                 'resting_ekg_results', 'slope_of_peak_exercise_st_segment']\n",
    "var_names_ord = ['num_major_vessels', 'chest_pain_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables numériques seront standardisées, mais la classe `StandardScaler` ne peut travailler que sur des nombres à virgule flottante. Transformez chacune des variables numérique de type `int64` en `float64` à l'aide de la méthode [pandas.DataFrame.astype](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['max_heart_rate_achieved'] = df.max_heart_rate_achieved.astype(np.float64)\n",
    "for col in var_names_num:\n",
    "    df[col] = df[col].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(dict([(var,np.float64) for var in var_names_num]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Construction du Jeu d'entraînement et du jeu de test\n",
    "\n",
    "Construisez un jeu d'entrainement et un jeu de test contenant 20% de notre dataset, pensez bien à séparer votre cible `heart_disease_present`. Pour la suite de ce notebook, vos variables devraient être nommées `X_train`, `y_train`, `X_test` et `y_test` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=77)\n",
    "X_train = train.drop(\"heart_disease_present\", axis=1)\n",
    "y_train = train[\"heart_disease_present\"].copy()\n",
    "X_test = test.drop(\"heart_disease_present\", axis=1)\n",
    "y_test = test[\"heart_disease_present\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Transformation des variables\n",
    "\n",
    "Nous allons créer une chaîne de transformation pour préparer nos données à l'aide de la classe [`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "Commencons par les features numériques : créez une instance de `Pipeline` en lui passant comme argument une liste composée d'un tuple de deux valeurs dont la première est un nom simple à retenir (par exemple : `std_scaler`) et la deuxième une instance de [`sklearn.preprocessing.StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Nommez cette instance `num_pipeline` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([('std_scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faites de même pour :\n",
    "* les features ordinales avec [`sklearn.preprocessing.OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html), nom de l'étape du Pipeline : `ord_encoder`, nom de l'instance `ord_pipeline`\n",
    "* les features catégorielles avec [`sklearn.preprocessing.OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) (passez la valeur `'ignore'` au paramètre `handle_unknown`), nom de l'étape du Pipeline : `1h_encoder`, nom de l'instance `cat_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ord_pipeline = Pipeline([('ord_encoder', OrdinalEncoder())])\n",
    "cat_pipeline = Pipeline([('1h_encoder', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez un pipeline pour la variable `age`, que vous nommerez `bin_discretizer` et que vous stockerez dans une variable `age_pipeline`, avec [`sklearn.preprocessing.KBinsDiscretizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html). Vous pouvez passer en paramètres `n_bins=6, strategy='uniform', encode='ordinal'` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "age_pipeline = Pipeline(\n",
    "    [\n",
    "        ('bin_discretizer', KBinsDiscretizer(n_bins=6,\n",
    "                                             strategy='uniform',\n",
    "                                             encode='ordinal'\n",
    "                                            )\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour finir nous allons composer nos Pipeline en indiquant sur quelles variables ils s'appliquent.\n",
    "\n",
    "Créez une instance de [`sklearn.compose.ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) en lui passant en paramètre une liste de tuples de trois valeurs. La première sera un nom simple à retenir (`num`, `ord`, `cat` et `age` par exemple), la deuxième votre variable contenant le pipeline correspondant et la dernière la liste des colonnes sur lesquels ce pipeline doit s'appliquer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preparation_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, var_names_num),\n",
    "    (\"ord\", ord_pipeline, var_names_ord),\n",
    "    (\"cat\", cat_pipeline, var_names_cat),\n",
    "    (\"age\", age_pipeline, ['age'])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisez l'effet de cette étape de transformation en l'appliquant sur `X_train` par exemple avec la méthode `fit_transform` (vous obtenez un tableau numpy, vous pouvez donc sélectionner la première ligne pour simplifier l'affichage) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preparation_pipeline.fit_transform(X_train)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Apprentissage d'une régression logistique\n",
    "\n",
    "Créez un Pipeline comprenant deux étapes :\n",
    "* vos pré-traitement, nommé `preparation`\n",
    "* une instance de [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), nommé `model` avec les paramètres `solver='liblinear', penalty='l2'` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_pipeline = Pipeline([\n",
    "    (\"preparation\", preparation_pipeline),\n",
    "    (\"model\", LogisticRegression(solver='liblinear', penalty='l2'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez une grille de paramètre pour ce pipeline où vous ferez varier le paramètre `C` de la régression logistique (`np.logspace(-5, 5, 10)` par exemple), et le paramètre `n_bins` de la binarisation de l'age (`[3,6,12]` par exemple). Les paramètres dans un pipeline sont accessible en précisant le nom de chaque sous-partie du pipeline séparé par deux underscores. Par exemple, `preparation__age__bin_discretizer__n_bins` permet de préciser que nous souhaitons faire varier le paramètre `n_bins` de `bin_discretizer`, lui même étant dans `age`, lui même dans `preparation` (d'où l'importance de bien nommé ses différentes étapes de pipeline) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'model__C': np.logspace(-5, 5, 10),\n",
    "        'preparation__age__bin_discretizer__n_bins': [3,6,12]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquez un grid search sur votre espace de recherche en utilisant la métrique de performance `f1` (paramètre `scoring` de `GridSearchCV` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    log_reg_pipeline, param_grid, cv=5,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez les meilleurs paramètres et le score correspondant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculez les scores avec une validation croisée pour le meilleur modèle sur le jeu d'entrainement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, scoring=\"f1\", cv=10)\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Apprentissage d'un forêt aléatoire\n",
    "\n",
    "Faites de même avec l'estimateur [`sklearn.ensemble.RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) (exemples pour le grid search : `max_depth` : `np.linspace(10,50,5, dtype=int)`, `min_samples_leaf` : `[2,4,8]`). Quel est le meilleur modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preparation\", preparation_pipeline),\n",
    "    ('model', RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=77))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__max_depth': np.linspace(10,50,5, dtype=int),\n",
    "    'model__min_samples_leaf': [2,4,8],\n",
    "    'preparation__age__bin_discretizer__n_bins': [3,6,12]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf_pipeline, param_grid, cv=5,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, scoring=\"f1\", cv=10)\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le meilleur modèle est obtenu avec une forêt aléatoire et les hyper-paramètres `max_depth = 10`, `min_samples_leaf = 4` et `age_bin_discretizer_n_bins = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Analyse des résultats pour le meilleur modèle\n",
    "\n",
    "Entrainer votre meilleur modèle sur tout le dataset d'entrainement, puis prédisez les classes pour votre dataset de test. Ensuite appliquez les fonctions `precision_score, recall_score, f1_score` du module [`sklearn.metrics`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics). Quel type d'erreurs fait votre classifieur ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Précision : \", precision_score(y_test, y_pred))\n",
    "print(\"Rappel : \", recall_score(y_test, y_pred))\n",
    "print(\"F1 : \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez la matrice de confusion avec [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). Vous pouvez obtenir un graphique de cette matrice avec [`sklearn.metrics.plot_confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html) ou [`sklearn.metrics.ConfusionMatrixDisplay.from_predictions`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(best_model, X_test, y_test)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "#rf_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Arbre de décision\n",
    "\n",
    "Le meilleur modèle obtenu est difficilement compréhensible (par des experts métier par exemple). Nous pouvons essayer d'entrainer un modèle plus simple et interprétable : un arbre de décision.\n",
    "\n",
    "Entrainez un arbre de décision sur les seules variables `max_heart_rate_achieved` et `oldpeak_eq_st_depression` avec une profondeur maximale de deux. Calculez la performance du modèle obtenu. Puis afficher cet arbre avec [`sklearn.tree.export_graphviz`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html). Vous pouvez enregistrer la sortie dans un fichier `.dot` et afficher le résultats avec le transformer en image avec le service : http://webgraphviz.com/\n",
    "\n",
    "Essayez d'améliorer le score obtenu avec un arbre de décision tout en conservant un modèle interprétable (features utilisées et profondeur maximale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = X_train[['max_heart_rate_achieved','oldpeak_eq_st_depression']]\n",
    "y = y_train\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=77)\n",
    "tree.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test[['max_heart_rate_achieved','oldpeak_eq_st_depression']])\n",
    "print(\"Précision : \", precision_score(y_test, y_pred))\n",
    "print(\"Rappel : \", recall_score(y_test, y_pred))\n",
    "print(\"F1 : \", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree,\n",
    "    out_file=\"heart_tree.dot\",\n",
    "    feature_names=['max_heart_rate_achieved','oldpeak_eq_st_depression'],\n",
    "    class_names=['malade', 'non malade'],\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier `.dot` généré par `export_graphviz` est transformé en image par le service : http://webgraphviz.com/\n",
    "\n",
    "![tree_graph](heart_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction suivante permet d'afficher les frontières de décision de l'arbre de décision sur notre dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(tree, X, y, axes):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    def make_grid_coord(x1_min, x1_max, x2_min, x2_max):\n",
    "        x1s = np.linspace(x1_min, x1_max, 100)\n",
    "        x2s = np.linspace(x2_min, x2_max, 100)\n",
    "        x1, x2 = np.meshgrid(x1s, x2s)\n",
    "        return (x1, x2, np.c_[x1.ravel(), x2.ravel()])\n",
    "\n",
    "    (x1, x2, X_new) = make_grid_coord(axes[0], axes[1], axes[2], axes[3])\n",
    "    y_pred = tree.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    plt.plot(X.iloc[:,0][y==0], X.iloc[:,1][y==0], \"yo\", label=\"Non\")\n",
    "    plt.plot(X.iloc[:,0][y==1], X.iloc[:,1][y==1], \"bs\", label=\"Oui\")\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree, X, y, [90, 210, -1, 7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
